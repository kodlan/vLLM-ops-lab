# =============================================================================
# vLLM Ops Lab - Environment Configuration
# =============================================================================
# Copy this file to .env and adjust values for your setup:
#   cp .env.example .env
# =============================================================================

# -----------------------------------------------------------------------------
# Model Configuration
# -----------------------------------------------------------------------------
# The HuggingFace model to serve. Use small models for experiments.
# Examples:
#   - Qwen/Qwen2.5-0.5B-Instruct (0.5B params, Apache 2.0, recommended)
#   - TinyLlama/TinyLlama-1.1B-Chat-v1.0 (1.1B params)
#   - microsoft/Phi-3-mini-4k-instruct (3.8B params, needs more VRAM)
MODEL_NAME=Qwen/Qwen2.5-0.5B-Instruct

# Maximum sequence length (prompt + response tokens).
# Higher values use more GPU memory. 4096 is a good default for small models.
VLLM_MAX_MODEL_LEN=4096

# Fraction of GPU memory vLLM can use (0.0 to 1.0).
# Lower values leave room for other processes. 0.8 is a safe default.
VLLM_GPU_MEMORY_UTILIZATION=0.8

# -----------------------------------------------------------------------------
# HuggingFace Configuration (Optional)
# -----------------------------------------------------------------------------
# Required only for gated models (e.g., Llama, Mistral).
# Get your token at: https://huggingface.co/settings/tokens
HF_TOKEN=